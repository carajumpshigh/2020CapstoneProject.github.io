<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic - Hyperspace by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">CUSP Capstone 2020</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="Simulations.html" class="active">Details</a></li>
<!--						<li><a href="elements.html">Elements</a></li>-->
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Simulations & Reinforcement Learning(RL)</h1>
							<p>While traversing the environment, the drones might have to map certain locations (cells/rasters) for a longer period of time due to the increased damage or noise in measurements, we call these locations with this type of uncertainty as anomalous. We used open-source tools to build a 2D and 3D simulation where the drones make decisions to cover a grid optimally. The major difference between the 2D and 3D simulation is the number of degrees of freedom the drones have. In the 3D simulation, the drones have 6 degrees of freedom whereas in the 2D simulation the drones only have 3 degrees of freedom. The difference in degrees of freedom increases the complexity of decisions that the drones have to make at each time step. Screenshots from the simulation are shown below. In order to start from a less complex task (lesser degrees of freedom) for the reinforcement learning algorithm, we train and evaluate our agents on the 2D simulation to begin with.</p>
							<p>The cells to be mapped by the drones are represented as grids. In the 2D simulation, the red dots represent anomalous cells and hence they requires multiple passes by the drones to reduce its uncertainty. Once all the cells are mapped by the drone swarm, the episode terminates. The agent gets a reward every time it successfully maps a cell and a relatively higher reward when it maps an anomalous cell. There is a negative reward, i.e a pun- ishment that exponentially increases per time step. This punishment forces the agent to terminate the episode as soon as possible, hence, covering the cells as fast as possible.</p>
							<p>We used Deep Q-Learning algorithm for training the agent which controls the drones. The agent has a Multi-Layer Perceptron (MLP) as a policy for each of the drone. These MLPs map a sparse representation of the input state to an action for each of the drone at a time-step. During training, the agent randomly explores the environment and collects information and stores them in a buffer which is then later sampled from at intervals and used to train the MLP. The Deep Q-learning algorithm iteratively approximates the Q function for each of the state and is established as a baseline for the learning based methods. The algorithm struggles to generalize and the drone moves to one side of the environment, i.e it always picks the same action every time. We describe this as a form of action collapse. To overcome this problem, we decided to use actor-critic methods with policy gradients to update the actor and critic network. The agent is trained using Advantage Actor-Critic (A2C) algorithm. The actor estimates an action given the state, the critic scales the reward function encouraging action with high rewards, the critic is subtracted by a baseline value which is the Value function, this reduces the noisiness in the signal and it is called advantage. Both Actor and Critic are Multi-Layer Perceptron network with memory state implemented using Gated Recurrent Unit (GRU). The agent provides a sample of probabilities over the set of discrete actions and we sample from that set for the agent to act. We observe that when we maintain the number of rollout steps and decrease the number of total steps the agent is trained, we are able to avoid the problem of action collapse. We hypothesize that this could be because as the number of steps increases, the drones memorize the most frequent action and repeats the same during the test time. This is similar to the problem of overfitting in machine learning.</p>

							<div class="left-2">
								<span class="image fit"><img src="images/s002.png" alt="" /></span>
							</div>
							<div class="right-2">
								<span class="image fit"><img src="images/s001%20copy.png" alt="" /></span>
							</div>

<!--							<div class="features">-->
<!--								<section>-->
<!--									<span class="image fit"><img src="images/s002.png" alt="" /></span>-->
<!--								</section>-->
<!--								<section>-->
<!--									<span class="image fit"><img src="images/s001.png" alt="" /></span>-->
<!--								</section>-->
<!--							</div>-->
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>Copyright &copy; 2020 All rights reserved..</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>